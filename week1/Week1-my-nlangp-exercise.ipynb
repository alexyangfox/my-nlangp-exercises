{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题  \n",
    "\n",
    "听起来很傻的问题，这里有两句话： \n",
    "\n",
    "- 'He saw their was a football in the park.'   \n",
    "- 'He saw there was a football in the park.'   \n",
    "\n",
    "哪句是对的呢？  \n",
    "\n",
    "我需要做的是，用 `First Order Markov Language Model` 和 `Second Order Markov Language Model` 分别求解出哪句话是对的。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# 如果你不想画图，是不需要 import plotly 的\n",
    "import plotly\n",
    "from plotly.graph_objs import Scatter, Layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = [word.lower() for word in nltk.corpus.brown.words()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_freq_dist = nltk.FreqDist(unigrams)\n",
    "unigrams_counts = unigrams_freq_dist.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "type": "scatter",
         "x": [
          "the",
          ",",
          ".",
          "of",
          "and",
          "to",
          "a",
          "in",
          "that",
          "is",
          "was",
          "he",
          "for",
          "``",
          "''",
          "it",
          "with",
          "as",
          "his",
          "on",
          "be",
          ";",
          "at",
          "by",
          "i",
          "this",
          "had",
          "?",
          "not",
          "are"
         ],
         "y": [
          69971,
          58334,
          49346,
          36412,
          28853,
          26158,
          23195,
          21337,
          10594,
          10109,
          9815,
          9548,
          9489,
          8837,
          8789,
          8760,
          7289,
          7253,
          6996,
          6741,
          6377,
          5566,
          5372,
          5306,
          5164,
          5145,
          5133,
          4693,
          4610,
          4394
         ]
        }
       ],
       "layout": {
        "title": "频数最高的 30 个 unigrams"
       }
      },
      "text/html": [
       "<div id=\"4025fade-ec51-4bb2-96f3-04db0c54d247\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"4025fade-ec51-4bb2-96f3-04db0c54d247\", [{\"x\": [\"the\", \",\", \".\", \"of\", \"and\", \"to\", \"a\", \"in\", \"that\", \"is\", \"was\", \"he\", \"for\", \"``\", \"''\", \"it\", \"with\", \"as\", \"his\", \"on\", \"be\", \";\", \"at\", \"by\", \"i\", \"this\", \"had\", \"?\", \"not\", \"are\"], \"y\": [69971, 58334, 49346, 36412, 28853, 26158, 23195, 21337, 10594, 10109, 9815, 9548, 9489, 8837, 8789, 8760, 7289, 7253, 6996, 6741, 6377, 5566, 5372, 5306, 5164, 5145, 5133, 4693, 4610, 4394], \"type\": \"scatter\"}], {\"title\": \"\\u9891\\u6570\\u6700\\u9ad8\\u7684 30 \\u4e2a unigrams\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"4025fade-ec51-4bb2-96f3-04db0c54d247\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"4025fade-ec51-4bb2-96f3-04db0c54d247\", [{\"x\": [\"the\", \",\", \".\", \"of\", \"and\", \"to\", \"a\", \"in\", \"that\", \"is\", \"was\", \"he\", \"for\", \"``\", \"''\", \"it\", \"with\", \"as\", \"his\", \"on\", \"be\", \";\", \"at\", \"by\", \"i\", \"this\", \"had\", \"?\", \"not\", \"are\"], \"y\": [69971, 58334, 49346, 36412, 28853, 26158, 23195, 21337, 10594, 10109, 9815, 9548, 9489, 8837, 8789, 8760, 7289, 7253, 6996, 6741, 6377, 5566, 5372, 5306, 5164, 5145, 5133, 4693, 4610, 4394], \"type\": \"scatter\"}], {\"title\": \"\\u9891\\u6570\\u6700\\u9ad8\\u7684 30 \\u4e2a unigrams\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "plotly.offline.iplot({\n",
    "    \"data\": [Scatter(x=[k for (k,v) in unigrams_counts[:30] ], y=[v for (k,v) in unigrams_counts[:30] ])],\n",
    "    \"layout\": Layout(title=\"频数最高的 30 个 unigrams\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'saw', 'their', 'was', 'a', 'football', 'in', 'the', 'park', '.']\n"
     ]
    }
   ],
   "source": [
    "s1 = 'He saw their was a football in the park.'  \n",
    "s2 = 'He saw there was a football in the park.'  \n",
    "\n",
    "tokens_in_s1 = [word.lower() for word in nltk.tokenize.wordpunct_tokenize(s1) ]\n",
    "tokens_in_s2 = [word.lower() for word in  nltk.tokenize.wordpunct_tokenize(s2) ]\n",
    "\n",
    "print(tokens_in_s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Order Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.ngrams(unigrams,2) # generator returned \n",
    "\n",
    "bigram_freq_dist = nltk.FreqDist(bigrams)\n",
    "bigrams_counts = bigram_freq_dist.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotly.offline.iplot({\n",
    "    \"data\": [Scatter(x=[k[0]+' '+k[1] for (k,v) in bigrams_counts[:30] ], y=[v for (k,v) in bigrams_counts[:30] ])],\n",
    "    \"layout\": Layout(title=\"频数最高的 30 个 bigrams\")\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('he', 'saw'), 0.00974026812842305), (('saw', 'there'), 0.0), (('there', 'was'), 0.2100441691564777), (('was', 'a'), 0.07916461224050571), (('a', 'football'), 0.0002586766616558999), (('football', 'in'), 0.0), (('in', 'the'), 0.2823735852574503), (('the', 'park'), 0.0002286663586193746), (('park', '.'), 0.1170213773726854)]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    [(bigram, bigram_freq_dist.freq(bigram)/ unigrams_freq_dist.freq(bigram[0])) for bigram in nltk.ngrams(tokens_in_s2,2) ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "遇到问题了， 不少 `bigrams` 的 `c(w0,w1)/c(w0)` 是 0 . \n",
    "\n",
    "接下来考虑一下 `discounting method`，这里让 `beta = 0.5`:  \n",
    "\n",
    "为了方便描述，定义两个集合：  \n",
    "\n",
    "- `集合A（w0）` ：  `{w1 ： c(w0,w1) > 0}`。 首先要注意，这个集合是单词的集合，或者说是 `unigram` 的集合，而不是 `bigram` 的集合。 这个集合表示的是，我们要考察的 bigram 是 `w0 ?`，也就是， `bigram` 里面的第一个词已经确定了，第二词还有很多种不同的可能。 对于每一种可能，如果在训练的文集里面，它的频数是大于零的，那么这个 bigram 中的第二个词语，就属于这个集合。   \n",
    "\n",
    "- `集合B（w0）` ：  `{w1 ： c(w0,w1) = 0}`， 这个集合表示的是，我们要考察的 bigram 是 `w0 ?`，也就是， `bigram` 里面的第一个词已经确定了，第二词还有很多种不同的可能。 对于每一种可能，如果在训练的文集里面，它的频数是等于零的，那么这个 bigram 中的第二个词语，就属于这个集合。 \n",
    "\n",
    "那么接下来看 `discounting method`: \n",
    "  \n",
    "-  $ q(w1|w0) = (c(w0,w1) - \\beta )/c(w0) $ ， 如果 `w1 属于 A(w0) 集合`.  \n",
    "-  `q(w1|w0) = alpha(w0,w1) * c(w1)/c(A(w0))`, 如果 `w1 属于 B（w0） 集合`,    \n",
    "- `c(B（w0）)`: 表示的是 `B（w0）` 集合里面，所有 unigrams 在训练文集里出现的频数之和。   \n",
    "- `alpha(w0,w1) 是 1-sum([q(w1|w0) for w1 属于 A(w0)])`    \n",
    "\n",
    "参考 [Week1-Required-Reading 的 16 页](Week1-required-reading.pdf)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.5\n",
    "def get_bigram_probability(bigram):\n",
    "    # bigram: (v,w)\n",
    "    # set A : {w : c(v,w) > 0} \n",
    "    # set B : {w : c(v,w) = 0}\n",
    "    v = bigram[0]\n",
    "    w = bigram[1]\n",
    "    c_v_w = bigram_freq_dist[bigram]\n",
    "    \n",
    "    if c_v_w > 0: \n",
    "        return (c_v_w - beta) / unigrams_freq_dist[v]\n",
    "    else:\n",
    "        # (v , *)： * 可能的单词有多少种？ \n",
    "        num_of_possible_word_after_v = 0;\n",
    "        for bi in bigram_freq_dist.keys(): \n",
    "            if bi[0] == v:\n",
    "                num_of_possible_word_after_v += 1\n",
    "        alpha = num_of_possible_word_after_v * beta / unigrams_freq_dist[v]\n",
    "        \n",
    "        set_A_total_counts = 0 \n",
    "        for bi in bigram_freq_dist.keys(): \n",
    "            if bi[0] == v:\n",
    "                set_A_total_counts += unigrams_freq_dist[bi[1]]\n",
    "        \n",
    "        return alpha*unigrams_freq_dist[w]/(unigrams_freq_dist.N() - set_A_total_counts)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好啦，现在来看看情况如何呢？ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('he', 'saw'), 0.009687892752408882), (('saw', 'their'), 0.0014204545454545455), (('their', 'was'), 0.002881783360951596), (('was', 'a'), 0.07911360163015792), (('a', 'football'), 0.0002371200689803837), (('football', 'in'), 0.006179374849610485), (('in', 'the'), 0.28234990860945774), (('the', 'park'), 0.00022152034414257336), (('park', '.'), 0.11170212765957446)]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    [(bigram, get_bigram_probability(bigram)) for bigram in nltk.ngrams(tokens_in_s1,2) ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('he', 'saw'), 0.009687892752408882), (('saw', 'there'), 0.0006517597167688712), (('there', 'was'), 0.20986070381231672), (('was', 'a'), 0.07911360163015792), (('a', 'football'), 0.0002371200689803837), (('football', 'in'), 0.006179374849610485), (('in', 'the'), 0.28234990860945774), (('the', 'park'), 0.00022152034414257336), (('park', '.'), 0.11170212765957446)]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    [(bigram, get_bigram_probability(bigram)) for bigram in nltk.ngrams(tokens_in_s2,2) ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import mul\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_s1 = reduce(mul,\n",
    "    [v for (k,v) in  [(bigram, get_bigram_probability(bigram)) for bigram in nltk.ngrams(tokens_in_s1,2) ]]\n",
    "    ,1)\n",
    "\n",
    "p_s2 = reduce(mul,\n",
    "    [v for (k,v) in  [(bigram, get_bigram_probability(bigram)) for bigram in nltk.ngrams(tokens_in_s2,2) ]]\n",
    "    ,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.211772068050607e-20\n",
      "1.0731852279363508e-18\n",
      "Aha! The right sentence is 'He saw there was a football in the park.'\n"
     ]
    }
   ],
   "source": [
    "print(p_s1)\n",
    "print(p_s2)\n",
    "if p_s1 < p_s2:\n",
    "    print('Aha! The right sentence is \\'' +\n",
    "          s2 + '\\''\n",
    "         )\n",
    "else:\n",
    "     print('Aha! The right sentence is \\'' +\n",
    "          s1 + '\\'' \n",
    "         )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Order Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = nltk.ngrams(unigrams,3) # generator returned \n",
    "trigram_freq_dist = nltk.FreqDist(trigrams) \n",
    "trigram_counts = trigram_freq_dist.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotly.offline.iplot({\n",
    "    \"data\": [Scatter(x=[k[0]+' '+k[1] + ' ' +k[2] for (k,v) in trigram_counts[:30] ], y=[v for (k,v) in trigram_counts[:30] ])],\n",
    "    \"layout\": Layout(title=\"频数最高的 30 个 trigrams\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](plot1.png)\n",
    "\n",
    "哈哈哈，从图上看来，没有去掉标点，真的是很多乱七八糟的东西啊。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我依然使用了和上面一样的 beta \n",
    "\n",
    "def get_trigram_probability(trigram):\n",
    "    # trigram: (u,v,w)\n",
    "    # set A : {w : c(u,v,w) > 0} \n",
    "    # set B : {w : c(u,v,w) = 0} \n",
    "    u = trigram[0]\n",
    "    v = trigram[1]\n",
    "    w = trigram[2]\n",
    "    counts_u_v_w = trigram_freq_dist[trigram]\n",
    "    if counts_u_v_w > 0:      \n",
    "        return (counts_u_v_w - beta) / bigram_freq_dist[(u,v)]     \n",
    "    else: \n",
    "        # (u,v,*) : 其中 * 这个单词有多少种可能， 或者说 w 有多少种可能 \n",
    "        num_of_possible_word_after_uv = 0\n",
    "        for tri in trigram_freq_dist.keys():\n",
    "            if tri[0]==u and tri[1]==v :\n",
    "                num_of_possible_word_after_uv += 1\n",
    "        if bigram_freq_dist[(u,v)] == 0:\n",
    "            alpha = 1 # back off to bigram \n",
    "        else:\n",
    "            alpha = num_of_possible_word_after_uv * beta / bigram_freq_dist[(u,v)]  \n",
    "        \n",
    "        # FIXME： 这里太慢了， 有什么更快的方法吗？\n",
    "        sum_of_Q_d_over_A = 0\n",
    "        for tri in trigram_freq_dist.keys():\n",
    "            if tri[0]==u and tri[1]==v :\n",
    "                sum_of_Q_d_over_A += get_bigram_probability((v,tri[2]))\n",
    "        denominator = 1 - sum_of_Q_d_over_A\n",
    "        \n",
    "        \n",
    "        return alpha*get_bigram_probability((w,v))/denominator\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('he', 'saw', 'their'), 5.458284880267648e-05), (('saw', 'their', 'was'), 0.0002816077858380414), (('their', 'was', 'a'), 2.155636990730761e-05), (('was', 'a', 'football'), 0.0019305019305019305), (('a', 'football', 'in'), 1.278189402105604e-05), (('football', 'in', 'the'), 2.1437452658958712e-05), (('in', 'the', 'park'), 0.0005809128630705395), (('the', 'park', '.'), 0.09375)]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    [(tri, get_trigram_probability(tri)) for tri in nltk.ngrams(tokens_in_s1,3) ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('he', 'saw', 'there'), 9.679839314667383e-05), (('saw', 'there', 'was'), 0.0036169128884360672), (('there', 'was', 'a'), 0.2469458987783595), (('was', 'a', 'football'), 0.0019305019305019305), (('a', 'football', 'in'), 1.278189402105604e-05), (('football', 'in', 'the'), 2.1437452658958712e-05), (('in', 'the', 'park'), 0.0005809128630705395), (('the', 'park', '.'), 0.09375)]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    [(tri, get_trigram_probability(tri)) for tri in nltk.ngrams(tokens_in_s2,3) ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_s1 = reduce(mul,\n",
    "    [v for (k,v) in  [(trigram, get_trigram_probability(trigram)) for trigram in nltk.ngrams(tokens_in_s1,3) ]]\n",
    "    ,1)\n",
    "\n",
    "p_s2 = reduce(mul,\n",
    "    [v for (k,v) in  [(trigram, get_trigram_probability(trigram)) for trigram in nltk.ngrams(tokens_in_s2,3) ]]\n",
    "    ,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.545471484009647e-30\n",
      "2.4907429831958207e-24\n",
      "Aha! The right sentence is 'He saw there was a football in the park.'\n"
     ]
    }
   ],
   "source": [
    "print(p_s1)\n",
    "print(p_s2)\n",
    "if p_s1 < p_s2:\n",
    "    print('Aha! The right sentence is \\'' +\n",
    "          s2 + '\\''\n",
    "         )\n",
    "else:\n",
    "     print('Aha! The right sentence is \\'' +\n",
    "          s1 + '\\'' \n",
    "         )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果把  `s1` 换成 `He saw the was a football in the park.` ， `s1` 会被认为是正确的句子， `Markov Model` 也是有问题的。 有的时候， `trigram` 或者 `bigram` 中的最后一个词与前面的词可能恰好关系是不大的，于是需要考虑 `grammars` 吗？  \n",
    "- 如果有出现 `unigram` 的 `counts` 也为 0 的情况该如何处理呢？  \n",
    "- 是否把 `trigram` 看成是 `bigram + unigram` ，在程序设计上会有一些很棒的思路呢？  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
